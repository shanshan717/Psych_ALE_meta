{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from glob import glob\n",
    "from math import sqrt\n",
    "from os import makedirs, path\n",
    "from re import sub\n",
    "from shutil import copy\n",
    "from sys import argv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nibabel import save\n",
    "from nilearn import image, reporting\n",
    "from nimare import correct, io, meta, utils\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our FSN simulations will heavily rely on the generation of so called \"null experiments,\" i.e., fictional experiments with their peaks randomly distributed across the brain. We'll start by writing a helper function for creating these. It takes as its input a \"real\" ALE data set (in the form of a Sleuth text file, see Notebook #1 and [this example](http://www.brainmap.org/ale/foci2.txt)). It then creates a desired number (`k_null`) of null experiments that are similar to the real experiments in terms of sample sizes and number of peak coordinates. However, the location of these peaks is drawn randomly from all voxels within our gray matter template. For these experiments, we know that the null hypothesis (i.e., no spatial convergence) is true, thus providing a testing ground for the file drawer effect.\n",
    "\n",
    "\n",
    "# Define function to generate a new data set with k null studies added\n",
    "def generate_null(\n",
    "    text_file=\"peaks.txt\", # 输入真实的数据集\n",
    "    space=\"mni152_2mm\",\n",
    "    k_null=100, # 生成的空数据集的数量,这里需要自己去确定一下默认值设置为多少比较合适\n",
    "    random_seed=None,\n",
    "    output_dir=\"./\", # 默认为当前工作目录\n",
    "):\n",
    "\n",
    "    # Load NiMARE's gray matter template\n",
    "    # 使用该函数加载指定分析空间的灰质模板\n",
    "    temp = utils.get_template(space=space, mask=\"gm\")\n",
    "\n",
    "    # Extract possible MNI coordinates for all gray matter voxels\n",
    "    # 提取可能的MNI坐标，这里是所有的灰质体素\n",
    "    # 第一行表示为找到灰质模板中所有的体素坐标\n",
    "    x, y, z = np.where(temp.get_fdata() == 1.0)\n",
    "    # 将坐标转换为mni空间\n",
    "    within_mni = image.coord_transform(x=x, y=y, z=z, affine=temp.affine)\n",
    "    # 转换数组，使其适用于后续处理\n",
    "    within_mni = np.array(within_mni).transpose()\n",
    "\n",
    "    # Read the original Sleuth file into a NiMARE data set\n",
    "    # 读取原始的Sleuth文件，并将其转换为NiMARE数据集\n",
    "    dset = io.convert_sleuth_to_dataset(text_file, target=space)\n",
    "\n",
    "    # Set a random seed to make the results reproducible\n",
    "    # 设置随机种子\n",
    "    if random_seed:\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    # Resample numbers of subjects per experiment based on the original data\n",
    "    # 重采样实验样本量\n",
    "    # 从原始数据集中提取样本量信息\n",
    "    nr_subjects_dset = [n[0] for n in dset.metadata[\"sample_sizes\"]]\n",
    "    # 通过 random.choices 随机选择样本量，获得 null 实验的样本数\n",
    "    nr_subjects_null = random.choices(nr_subjects_dset, k=k_null)\n",
    "\n",
    "    # Resample numbers of peaks per experiment based on the original data\n",
    "    # 重采样实验中的峰值数量\n",
    "    nr_peaks_dset = dset.coordinates[\"study_id\"].value_counts().tolist()\n",
    "    # 通过 random.choices 随机选择峰值数量，获得 null 实验的峰值数\n",
    "    nr_peaks_null = random.choices(nr_peaks_dset, k=k_null)\n",
    "\n",
    "    # Create random peak coordinates\n",
    "    # 创建随机峰值坐标\n",
    "    idx_list = [\n",
    "        random.sample(range(len(within_mni)), k=k_peaks) for k_peaks in nr_peaks_null\n",
    "    ]\n",
    "    peaks_null = [within_mni[idx] for idx in idx_list] # 将选择的坐标存入 peaks_null列表里\n",
    "\n",
    "    # Copy original experiments to the destination Sleuth file\n",
    "    # 复制原始实验到新的sleuth文件中\n",
    "    makedirs(output_dir, exist_ok=True)\n",
    "    text_file_basename = path.basename(text_file)\n",
    "    # 将原始数据集复制到新的文件中，以便在其中追加null实验\n",
    "    null_file_basename = sub(\n",
    "        pattern=\".txt\", repl=\"_plus_k\" + str(k_null) + \".txt\", string=text_file_basename\n",
    "    )\n",
    "    null_file = output_dir + \"/\" + null_file_basename\n",
    "    copy(text_file, null_file)\n",
    "\n",
    "    # Append all the null studies to the Sleuth file\n",
    "    # 将null实验追加到新的sleuth文件中\n",
    "    # 打开新创建的sleuth文件\n",
    "    # 循环k_null次，将null实验的信息写入文件中，然后将随机生成的峰值坐标写入文件中\n",
    "    f = open(null_file, mode=\"a\")\n",
    "    for i in range(k_null):\n",
    "        f.write(\n",
    "            \"\\n// nullstudy\"\n",
    "            + str(i + 1)\n",
    "            + \"\\n// Subjects=\"\n",
    "            + str(nr_subjects_null[i])\n",
    "            + \"\\n\"\n",
    "        )\n",
    "        np.savetxt(f, peaks_null[i], fmt=\"%.3f\", delimiter=\"\\t\")\n",
    "    f.close() # 关闭文件\n",
    "\n",
    "    # Read the new Sleuth file and return it as a NiMARE data set\n",
    "    # 读取新的sleuth文件，并将其转换为NiMARE数据集\n",
    "    dset_null = io.convert_sleuth_to_dataset(null_file, target=space)\n",
    "    # 返回新的数据集\n",
    "    return dset_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this helper function, we can go on to implement the actual FSN simulation. This function will be rather long-winded and complex, but the overall logic is simple: Take the Sleuth file from the original ALE analysis, generate a large number of null experimetns, and add them iteratively to the analysis. At each step, re-estimate the ALE and record which voxel have remained statistically significant and which have not. Based on this, each (initially singificant) voxel will be assigned an FSN value which is equal to the highest number of null experiments that we were able to add before the voxel failed to reach significant for the first time. Because these simulations are going to take a really (!) long time, we implement two stopping rules: We either stop if there are no more significant voxels left *or* if we've added a sufficiently large number of null studies for our purpose (e.g., five times times the number of original studies in the meta-analysis).\n",
    "\n",
    "# Define function to compute the FSN for all voxels from a Sleuth file\n",
    "# compute_fsn 函数用于计算所有体素的FSN值\n",
    "def compute_fsn(\n",
    "    text_file=\"peaks.txt\",\n",
    "    space=\"mni152_2mm\",\n",
    "    voxel_thresh=0.001,\n",
    "    cluster_thresh=0.01,\n",
    "    n_iters=1000,\n",
    "    k_max_factor=5,\n",
    "    random_ale_seed=None,\n",
    "    random_null_seed=None,\n",
    "    output_dir=\"./\",\n",
    "):\n",
    "\n",
    "    # Let's show the user what we are doing\n",
    "    print(\"\\nCOMPUTING FSN FOR \" + text_file + \" (seed: \" + str(random_null_seed) + \")\")\n",
    "\n",
    "    # Set random seed for original ALE if requested\n",
    "    if random_ale_seed:\n",
    "        np.random.seed(random_ale_seed)\n",
    "\n",
    "    # Recreate the original ALE analysis\n",
    "    # 重新创建原始的ALE分析\n",
    "    # 创建ALE实例ale\n",
    "    ale = meta.cbma.ALE()\n",
    "    corr = correct.FWECorrector(\n",
    "        method=\"montecarlo\", voxel_thresh=voxel_thresh, n_iters=n_iters\n",
    "    )\n",
    "    dset_orig = io.convert_sleuth_to_dataset(text_file=text_file, target=space)\n",
    "    res_orig = ale.fit(dset_orig)\n",
    "    cres_orig = corr.transform(res_orig)\n",
    "\n",
    "    # Extract the original study IDs\n",
    "    # 提取原始研究的ID\n",
    "    ids_orig = dset_orig.ids.tolist()\n",
    "\n",
    "    # Create a new data set with a large number null studies added\n",
    "    # 创建包含大量虚构实验的数据集\n",
    "    # 计算最大虚构实验数量 k_max，为原始研究数量的 k_max_factor 倍\n",
    "    k_max = len(ids_orig) * k_max_factor\n",
    "    # 调用 generate_null 函数生成包含大量虚构实验的数据集 dset_null\n",
    "    dset_null = generate_null(\n",
    "        text_file=text_file,\n",
    "        space=space,\n",
    "        k_null=k_max,\n",
    "        random_seed=random_null_seed,\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "\n",
    "    # Create thresholded cluster mask\n",
    "    # 创建聚类阈值掩膜\n",
    "    img_fsn = cres_orig.get_map(\"z_level-cluster_corr-FWE_method-montecarlo\")\n",
    "    cluster_thresh_z = norm.ppf(1 - cluster_thresh / 2)\n",
    "    img_fsn = image.threshold_img(img_fsn, threshold=cluster_thresh_z)\n",
    "    img_fsn = image.math_img(\"np.where(img > 0, 1, 0)\", img=img_fsn)\n",
    "\n",
    "    # Create cluster-thresholded z map\n",
    "    # 创建聚类阈值z map\n",
    "    img_z = cres_orig.get_map(\"z\")\n",
    "    img_z = image.math_img(\"img1 * img2\", img1=img_fsn, img2=img_z)\n",
    "\n",
    "    # Iteratively add null studies up to our pre-defined maximum\n",
    "    # 开始一个循环，迭代添加null studies，直到达到预定义的最大值\n",
    "    for k in range(1, k_max):\n",
    "\n",
    "        # Print message\n",
    "        print(\"Computing ALE for k = \" + str(k) + \" null studies added...\")\n",
    "\n",
    "        # Create a new data set with k null studies added\n",
    "        ids_null = [\"nullstudy\" + str(x) + \"-\" for x in range(1, k + 1)]\n",
    "        ids = ids_orig + ids_null\n",
    "        dset_k = dset_null.slice(ids)\n",
    "\n",
    "        # Compute the ALE\n",
    "        res_k = res = ale.fit(dset_k)\n",
    "        cres_k = corr.transform(result=res_k)\n",
    "\n",
    "        # Create a thresholded cluster mask\n",
    "        img_k = cres_k.get_map(\"z_level-cluster_corr-FWE_method-montecarlo\")\n",
    "        img_k = image.threshold_img(img_k, threshold=cluster_thresh_z)\n",
    "        img_k = image.math_img(\"np.where(img > 0, 1, 0)\", img=img_k)\n",
    "\n",
    "        # Use this to update the per-voxel FSN - this is a bit hack-ish: On a voxel-by-\n",
    "        # voxel basis, we increase the value by 1 if and only if the voxel has remained\n",
    "        # significant. As soon as it has failed to reach significance once, we never\n",
    "        # increase FSN any further. This is handeled by comparing the current FSN to\n",
    "        # the current value of k.\n",
    "        # 更新每个体素的FSN值\n",
    "        count = str(k + 1)\n",
    "        formula = \"np.where(img_fsn + img_k == \" + count + \", img_fsn + 1, img_fsn)\"\n",
    "        img_fsn = image.math_img(formula, img_fsn=img_fsn, img_k=img_k)\n",
    "\n",
    "        # Quit as soon as there are no significant clusters left in the current map\n",
    "        if not np.any(img_k.get_fdata()):\n",
    "            print(\"No more significant voxels - terminating\\n\")\n",
    "            break\n",
    "\n",
    "    # Save the FSN map that we've created in the loop\n",
    "    # 保存在循环中创建的FSN map\n",
    "    filename_img = path.basename(text_file).replace(\".txt\", \"_fsn.nii.gz\")\n",
    "    save(img_fsn, filename=output_dir + \"/\" + filename_img)\n",
    "\n",
    "    # Extract the FSN values at the original cluster peaks\n",
    "    # 提取原始聚类峰值处的FSN值\n",
    "    tab_fsn = reporting.get_clusters_table(img_z, stat_threshold=0, min_distance=1000)\n",
    "    inv_affine = np.linalg.inv(img_z.affine)\n",
    "    x, y, z = [np.array(tab_fsn[col]) for col in [\"X\", \"Y\", \"Z\"]]\n",
    "    x, y, z = image.coord_transform(x=x, y=y, z=z, affine=inv_affine)\n",
    "    x, y, z = [arr.astype(\"int\") for arr in [x, y, z]]\n",
    "    tab_fsn[\"FSN\"] = img_fsn.get_fdata()[x, y, z]\n",
    "\n",
    "    # Save this cluster table with the new FSN column\n",
    "    # 保存包含新FSN列的聚类表\n",
    "    filename_tab = path.basename(text_file).replace(\".txt\", \"_fsn.tsv\")\n",
    "    tab_fsn.to_csv(output_dir + \"/\" + filename_tab, sep=\"\\t\", index=False)\n",
    "\n",
    "    return img_fsn, tab_fsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMPUTING FSN FOR /Users/ss/Documents/Psych_ALE_meta/data/Self_all.txt (seed: 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.correct:Using correction method implemented in Estimator: nimare.meta.cbma.ale.ALE.correct_fwe_montecarlo.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffacc7b6ee4415d82ede321000fa4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.meta.cbma.base:Using null distribution for voxel-level FWE correction.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mask option 'gm' not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m\n\u001b[1;32m     24\u001b[0m filedrawers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiledrawer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(seed) \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m random_null_seeds]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# The actual simulations are happening here: For each of the input text file (one for each original ALE analysis), we compute one FSN map for a number of different filedrawers (in our paper, we used ten). The results for each file drawer will be stored in separate folder which also indicates the random seed value (e.g., `166`) that was used to generate the null experiments (e.g., `results/fsn/all/filedrawer166`).\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Use our function to compute multiple filedrawers for each text file\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 执行多次FSN计算，每个文件夹包含一个文件抽屉的结果\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 进行实际的 FSN 计算模拟：对于每个输入文本文件（每个原始 ALE 分析），为多个不同的文件夹计算一个 FSN 映射。\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 通过双重列表推导式，引入 compute_fsn 函数，遍历每个文本文件及其对应的输出目录，传入相应的随机种子。\u001b[39;00m\n\u001b[1;32m     33\u001b[0m _ \u001b[38;5;241m=\u001b[39m [  \n\u001b[1;32m     34\u001b[0m     [  \n\u001b[0;32m---> 35\u001b[0m         compute_fsn(  \n\u001b[1;32m     36\u001b[0m             text_file\u001b[38;5;241m=\u001b[39mtext_files,   \n\u001b[1;32m     37\u001b[0m             space\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmni152_2mm\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m     38\u001b[0m             voxel_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,  \n\u001b[1;32m     39\u001b[0m             cluster_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,  \n\u001b[1;32m     40\u001b[0m             n_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,  \n\u001b[1;32m     41\u001b[0m             k_max_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \n\u001b[1;32m     42\u001b[0m             random_ale_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m,  \n\u001b[1;32m     43\u001b[0m             random_null_seed\u001b[38;5;241m=\u001b[39mrandom_null_seed,  \n\u001b[1;32m     44\u001b[0m             output_dir\u001b[38;5;241m=\u001b[39moutput_dir \u001b[38;5;241m+\u001b[39m filedrawer,  \n\u001b[1;32m     45\u001b[0m         )  \n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m random_null_seed, filedrawer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(random_null_seeds, filedrawers)  \n\u001b[1;32m     47\u001b[0m     ]  \n\u001b[1;32m     48\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mcompute_fsn\u001b[0;34m(text_file, space, voxel_thresh, cluster_thresh, n_iters, k_max_factor, random_ale_seed, random_null_seed, output_dir)\u001b[0m\n\u001b[1;32m     42\u001b[0m k_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids_orig) \u001b[38;5;241m*\u001b[39m k_max_factor\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 调用 generate_null 函数生成包含大量虚构实验的数据集 dset_null\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m dset_null \u001b[38;5;241m=\u001b[39m generate_null(\n\u001b[1;32m     45\u001b[0m     text_file\u001b[38;5;241m=\u001b[39mtext_file,\n\u001b[1;32m     46\u001b[0m     space\u001b[38;5;241m=\u001b[39mspace,\n\u001b[1;32m     47\u001b[0m     k_null\u001b[38;5;241m=\u001b[39mk_max,\n\u001b[1;32m     48\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39mrandom_null_seed,\n\u001b[1;32m     49\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Create thresholded cluster mask\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 创建聚类阈值掩膜\u001b[39;00m\n\u001b[1;32m     54\u001b[0m img_fsn \u001b[38;5;241m=\u001b[39m cres_orig\u001b[38;5;241m.\u001b[39mget_map(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_level-cluster_corr-FWE_method-montecarlo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m, in \u001b[0;36mgenerate_null\u001b[0;34m(text_file, space, k_null, random_seed, output_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_null\u001b[39m(\n\u001b[1;32m      6\u001b[0m     text_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeaks.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# 输入真实的数据集\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     space\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmni152_2mm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Load NiMARE's gray matter template\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# 使用该函数加载指定分析空间的灰质模板\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     temp \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_template(space\u001b[38;5;241m=\u001b[39mspace, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Extract possible MNI coordinates for all gray matter voxels\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# 提取可能的MNI坐标，这里是所有的灰质体素\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# 第一行表示为找到灰质模板中所有的体素坐标\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     x, y, z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(temp\u001b[38;5;241m.\u001b[39mget_fdata() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nimare/utils.py:99\u001b[0m, in \u001b[0;36mget_template\u001b[0;34m(space, mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m         img \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m     96\u001b[0m             op\u001b[38;5;241m.\u001b[39mjoin(template_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpl-MNI152NLin6Asym_res-02_desc-brain_mask.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask option \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m space \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124male_2mm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Mask option 'gm' not supported"
     ]
    }
   ],
   "source": [
    "# Ideally, we want to perform all of this multiple times for different (random) filedrawers. Otherwise, the resulting FSN values would hinge a lot on the random patterns of these specific null experiments. However, doing all of these iterative simulations multiple times is extremly computationally expensive. We therefore wrote the next bit of the notebook in a way so that it can be run in parallel on our high performance computing (HPC) cluster. For this, we would call this notebook as a Python script from the command line and need to provide it with two additional parameters: The name of the original ALE analysis for which we want to compute the FSN and the number of different filedrawers we want to estimate (so we can always compute multiple filedrawers in parallel). If you don't happen to have access to an HPC and want to try the out the simulations directly withing the notebook, simply uncomment the two lines of code to define `prefixes` and `nr_filedrawers` locally.\n",
    "\n",
    "\n",
    "\n",
    "# # Or define them here for debugging\n",
    "# prefixes = [\"all\", \"knowledge\", \"relatedness\", \"objects\"]\n",
    "# nr_filedrawers = 10\n",
    "# 为调试目的可以手动定义 prefixes 和 nr_filedrawers\n",
    "nr_filedrawers = 10\n",
    "\n",
    "# List the filenames of the Sleuth text files\n",
    "# 根据前缀构建包含原始 ALE 数据集的文本文件列表 text_files\n",
    "# text_files = [\"../results/ale/\" + prefix + \".txt\" for prefix in prefixes]\n",
    "text_files = \"/Users/ss/Documents/Psych_ALE_meta/data/Self_all.txt\" \n",
    "\n",
    "# Create output directory names\n",
    "# 为不同的分析前缀创建相应的输出目录列表 output_dirs\n",
    "output_dir = \"../results/fsn/\"\n",
    "\n",
    "# Create random seeds for filedrawers\n",
    "# 生成指定数量的随机种子 random_null_seeds，以保证每个文件夹的随机实验不同\n",
    "# 创建文件夹名称列表 filedrawers，格式为 filedrawer 加上随机种子\n",
    "random_null_seeds = random.sample(range(1000), k=nr_filedrawers)\n",
    "filedrawers = [\"filedrawer\" + str(seed) for seed in random_null_seeds]\n",
    "\n",
    "# The actual simulations are happening here: For each of the input text file (one for each original ALE analysis), we compute one FSN map for a number of different filedrawers (in our paper, we used ten). The results for each file drawer will be stored in separate folder which also indicates the random seed value (e.g., `166`) that was used to generate the null experiments (e.g., `results/fsn/all/filedrawer166`).\n",
    "\n",
    "# Use our function to compute multiple filedrawers for each text file\n",
    "# 执行多次FSN计算，每个文件夹包含一个文件抽屉的结果\n",
    "# 进行实际的 FSN 计算模拟：对于每个输入文本文件（每个原始 ALE 分析），为多个不同的文件夹计算一个 FSN 映射。\n",
    "# 通过双重列表推导式，引入 compute_fsn 函数，遍历每个文本文件及其对应的输出目录，传入相应的随机种子。\n",
    "\n",
    "_ = [  \n",
    "    [  \n",
    "        compute_fsn(  \n",
    "            text_file=text_files,   \n",
    "            space=\"mni152_2mm\",  \n",
    "            voxel_thresh=0.001,  \n",
    "            cluster_thresh=0.01,  \n",
    "            n_iters=100,  \n",
    "            k_max_factor=5,  \n",
    "            random_ale_seed=1234,  \n",
    "            random_null_seed=random_null_seed,  \n",
    "            output_dir=output_dir + filedrawer,  \n",
    "        )  \n",
    "        for random_null_seed, filedrawer in zip(random_null_seeds, filedrawers)  \n",
    "    ]  \n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the simulation are done, we need to aggregate the results that we've obtained for each analysis across multiple file drawers. Remember that for each file drawer, we've already stored an FSN map as well as a table that contains the FSN value for each of the original cluster peaks. Here we just average these maps and perform some summary statistics on the tables, such as computing the mean FSN and its 95% confidence interval across the multiple file drawers (in our case, ten).\n",
    "# 汇总结果\n",
    "\n",
    "# Compute mean FSN across filedrawers\n",
    "# 计算多个文件抽屉的平均FSN\n",
    "for prefix in prefixes:\n",
    "\n",
    "    # Read FSN maps from all filedrawers\n",
    "    fnames_maps = glob(\n",
    "        \"../results/fsn/\" + prefix + \"/filedrawer*/\" + prefix + \"_fsn.nii.gz\"\n",
    "    )\n",
    "    imgs_fsn = [image.load_img(fname) for fname in fnames_maps]\n",
    "\n",
    "    # Average and save\n",
    "    # 计算并保存均值FSN 图像\n",
    "    img_mean = image.mean_img(imgs_fsn)\n",
    "    fname_img_mean = \"../results/fsn/\" + prefix + \"/\" + prefix + \"_mean_fsn.nii.gz\"\n",
    "    save(img_mean, fname_img_mean)\n",
    "\n",
    "    # Read FSN tables from all filedrawers\n",
    "    # 读取FSN 表格\n",
    "    fnames_tabs = glob(\n",
    "        \"../results/fsn/\" + prefix + \"/filedrawer*/\" + prefix + \"_fsn.tsv\"\n",
    "    )\n",
    "    tabs_fsn = [pd.read_csv(fname, delimiter=\"\\t\") for fname in fnames_tabs]\n",
    "    tab_fsn = pd.concat(tabs_fsn)\n",
    "\n",
    "    # Compute summary statistics\n",
    "    # 计算聚合统计信息\n",
    "    agg = tab_fsn.groupby(\"Cluster ID\")[\"FSN\"].agg([\"mean\", \"count\", \"std\"])\n",
    "\n",
    "    # Compute confidence intervals\n",
    "    # 计算置信区间\n",
    "    # 定义置信水平 ci_level 为 0.05（对应 95% 置信水平），并使用正态分布的逆累积分布函数 norm.ppf() 计算 z 临界值 z_crit\n",
    "    ci_level = 0.05\n",
    "    z_crit = abs(norm.ppf(ci_level / 2))\n",
    "    # 计算标准误差 se、置信区间下限 ci_lower 和上限\n",
    "    agg[\"se\"] = [std / sqrt(count) for std, count in zip(agg[\"std\"], agg[\"count\"])]\n",
    "    agg[\"ci_lower\"] = agg[\"mean\"] - z_crit * agg[\"se\"]\n",
    "    agg[\"ci_upper\"] = agg[\"mean\"] + z_crit * agg[\"se\"]\n",
    "\n",
    "    # Save summary statistics\n",
    "    # 保存总结统计信息\n",
    "    fname_agg = \"../results/fsn/\" + prefix + \"/\" + prefix + \"_mean_fsn.csv\"\n",
    "    agg.to_csv(fname_agg, float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取所有文件抽屉的FSN图像  \n",
    "fnames_maps = glob(\"../results/fsn/filedrawer*/_fsn.nii.gz\")  \n",
    "imgs_fsn = [image.load_img(fname) for fname in fnames_maps]  \n",
    "\n",
    "# 计算并保存均值FSN图像  \n",
    "img_mean = image.mean_img(imgs_fsn)  \n",
    "fname_img_mean = \"../results/fsn/mean_fsn.nii.gz\"  \n",
    "image.save_img(img_mean, fname_img_mean)  \n",
    "\n",
    "# 读取FSN表格  \n",
    "fnames_tabs = glob(\"../results/fsn/filedrawer*/_fsn.tsv\")  \n",
    "tabs_fsn = [pd.read_csv(fname, delimiter=\"\\t\") for fname in fnames_tabs]  \n",
    "tab_fsn = pd.concat(tabs_fsn)  \n",
    "\n",
    "# 计算聚合统计信息  \n",
    "agg = tab_fsn.groupby(\"Cluster ID\")[\"FSN\"].agg([\"mean\", \"count\", \"std\"])  \n",
    "\n",
    "# 计算置信区间  \n",
    "ci_level = 0.05  \n",
    "z_crit = abs(norm.ppf(ci_level / 2))  \n",
    "agg[\"se\"] = [std / sqrt(count) for std, count in zip(agg[\"std\"], agg[\"count\"])]  \n",
    "agg[\"ci_lower\"] = agg[\"mean\"] - z_crit * agg[\"se\"]  \n",
    "agg[\"ci_upper\"] = agg[\"mean\"] + z_crit * agg[\"se\"]  \n",
    "\n",
    "# 保存总结统计信息  \n",
    "fname_agg = \"../results/fsn/mean_fsn.csv\"  \n",
    "agg.to_csv(fname_agg, float_format=\"%.3f\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
