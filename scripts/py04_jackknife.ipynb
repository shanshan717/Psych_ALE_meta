{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from nibabel import save\n",
    "from nilearn import image, plotting, reporting\n",
    "from nimare import correct, io, meta\n",
    "from scipy.stats import norm\n",
    "\n",
    "# %% [markdown]\n",
    "# Because we want to perform the jackknife analysis for multiple ALEs (i.e., the main analysis and the three task category-specific analyses), let's again define a helper function. This takes as its input a Sleuth text file (the same that we've created in Notebook #01 to run the original ALE analysis) plus a couple of additional parameters that will be used for all ALE simulations. These, of course, should be the same as for the original analysis so that the jackknife results will be meaningful.\n",
    "#\n",
    "# The logic of our function is to read the Sleuth file and re-create the original ALE analysis (with all experiments inlcuded). Then, it loops over all those experiments and, at each iteration, drops the current one from the sample and re-estimates the ALE. The resulting meta-analytic map is converted into a binary mask telling us which voxels remained statistically significant ($0$ = not signficant, $1$ = significant). Once we've done this for all experiments, the images are averaged into a \"jackknife map.\" This simply shows us for each cluster the percentage of simulations in which it has remained significant. It can therefore be seen as an indicator for the robustness of the cluster against spurious results in the meta-analytic sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function to perform a single jackknife analysis\n",
    "def compute_jackknife(\n",
    "    text_file=\"peaks.txt\", # 包含元分析峰值数据的slueth文件\n",
    "    space=\"mni152_2mm\", # 还是使用mni152_2mm空间\n",
    "    voxel_thresh=0.001,\n",
    "    cluster_thresh=0.01,\n",
    "    n_iters=1000,\n",
    "    random_seed=None,\n",
    "    output_dir=\"./\",\n",
    "):\n",
    "\n",
    "    # Set random seeds if requested\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Create NiMARE data set from the Sleuth file\n",
    "    dset_orig = io.convert_sleuth_to_dataset(text_file)\n",
    "    study_ids = dset_orig.ids\n",
    "\n",
    "    # Specify ALE and FWE transformers\n",
    "    ale = meta.cbma.ALE()\n",
    "    corr = correct.FWECorrector(\n",
    "        method=\"montecarlo\", voxel_thresh=voxel_thresh, n_iters=n_iters\n",
    "    )\n",
    "\n",
    "    # Create output folder\n",
    "    _ = makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create empty list to store the jackknife'd cluster images\n",
    "    # 初始化空列表imgs_jk，用于存储每次jackknife的生成的聚类图像\n",
    "    imgs_jk = []\n",
    "\n",
    "    for study_id in study_ids:\n",
    "\n",
    "        # Create new data set with the current study removed\n",
    "        # 移除当前实验study_id，以便生成新的实验ID列表study_ids_jk  \n",
    "        study_ids_jk = study_ids[study_ids != study_id]\n",
    "\n",
    "        # Fit the jackknife'd ALE\n",
    "        # 创建仅包含study_ids_jk的数据集dset_jk，然后拟合ALE模型\n",
    "        dset_jk = dset_orig.slice(study_ids_jk)\n",
    "        res_jk = ale.fit(dset_jk)\n",
    "        # 并应用FWE校正\n",
    "        cres_jk = corr.transform(res_jk)\n",
    "\n",
    "        # Create and save the thresholded cluster mask\n",
    "        # 生成阈值化的聚类掩模\n",
    "        img_jk = cres_jk.get_map(\"z_level-cluster_corr-FWE_method-montecarlo\")\n",
    "        cluster_thresh_z = norm.ppf(1 - cluster_thresh / 2)\n",
    "        formula = \"np.where(img > \" + str(cluster_thresh_z) + \", 1, 0)\"\n",
    "        img_jk = image.math_img(formula, img=img_jk)\n",
    "\n",
    "        # Save to the output folder and to our list\n",
    "        study_id_short = study_id.replace(\"-\", \"\")\n",
    "        save(img_jk, filename=output_dir + \"/jk_\" + study_id_short + \".nii.gz\")\n",
    "        imgs_jk.append(img_jk)\n",
    "\n",
    "    # Create and save averaged jackknife image\n",
    "    img_mean = image.mean_img(imgs_jk)\n",
    "    save(img_mean, filename=output_dir + \"/mean_jk.nii.gz\")\n",
    "\n",
    "    return img_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So far we've only defined the jackknife function; now let's apply it to our ALE analyses. We just need to list the Sleuth text file names and provide our default ALE thresholds and settings. The function will run for a couple of hours and return, for each input Sleuth file, an averaged jackknife map.\n",
    "\n",
    "# List the Sleuth files for which to run a jackknife analysis\n",
    "# 生成包含所有实验的Sleuth文件\n",
    "# prefixes = [\"all\", \"knowledge\", \"relatedness\", \"objects\"]\n",
    "# text_files = [\"../results/ale/\" + prefix + \".txt\" for prefix in prefixes]\n",
    "\n",
    "# # Create output directory names\n",
    "# output_dirs = [\"../results/jackknife/\" + prefix for prefix in prefixes]\n",
    "\n",
    "text_file = \"/Users/ss/Documents/Psych_ALE_meta/data/Self_all.txt\"\n",
    "\n",
    "output_dir = \"../results/jackknife/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.correct:Using correction method implemented in Estimator: nimare.meta.cbma.ale.ALE.correct_fwe_montecarlo.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5d477c61bd40ae898555b2664dd98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.meta.cbma.base:Using null distribution for voxel-level FWE correction.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No map with name 'z_level-cluster_corr-FWE_method-montecarlo' found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the jackknife function\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m jk \u001b[38;5;241m=\u001b[39m compute_jackknife(  \n\u001b[1;32m      4\u001b[0m     text_file\u001b[38;5;241m=\u001b[39mtext_file,  \n\u001b[1;32m      5\u001b[0m     space\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmni152_2mm\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m      6\u001b[0m     voxel_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,  \n\u001b[1;32m      7\u001b[0m     cluster_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,  \n\u001b[1;32m      8\u001b[0m     n_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,  \n\u001b[1;32m      9\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m,  \n\u001b[1;32m     10\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,  \n\u001b[1;32m     11\u001b[0m )\n",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m, in \u001b[0;36mcompute_jackknife\u001b[0;34m(text_file, space, voxel_thresh, cluster_thresh, n_iters, random_seed, output_dir)\u001b[0m\n\u001b[1;32m     44\u001b[0m cres_jk \u001b[38;5;241m=\u001b[39m corr\u001b[38;5;241m.\u001b[39mtransform(res_jk)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Create and save the thresholded cluster mask\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 生成阈值化的聚类掩模\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m img_jk \u001b[38;5;241m=\u001b[39m cres_jk\u001b[38;5;241m.\u001b[39mget_map(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_level-cluster_corr-FWE_method-montecarlo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m cluster_thresh_z \u001b[38;5;241m=\u001b[39m norm\u001b[38;5;241m.\u001b[39mppf(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m cluster_thresh \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     50\u001b[0m formula \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.where(img > \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(cluster_thresh_z) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, 1, 0)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nimare/results.py:132\u001b[0m, in \u001b[0;36mMetaResult.get_map\u001b[0;34m(self, name, return_type)\u001b[0m\n\u001b[1;32m    130\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaps\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo map with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# pending resolution of https://github.com/nilearn/nilearn/issues/2724\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: No map with name 'z_level-cluster_corr-FWE_method-montecarlo' found."
     ]
    }
   ],
   "source": [
    "# Apply the jackknife function\n",
    "\n",
    "jk = compute_jackknife(  \n",
    "    text_file=text_file,  \n",
    "    space=\"mni152_2mm\",  \n",
    "    voxel_thresh=0.001,  \n",
    "    cluster_thresh=0.05,  \n",
    "    n_iters=5000,  \n",
    "    random_seed=1234,  \n",
    "    output_dir=output_dir,  \n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's examine one of these maps, here for our main analysis containing all semantic knowledge experiments. Note that most clusters have a jackknife reliability of $1.0$, indicating strong robustness against the deletion of individual experiments. Only for two of the posterior clusters in the right hemisphere, the robustness is slightly reduced (but still high at approx. $0.85$).\n",
    "\n",
    "# Glass brain example\n",
    "img_jk_all = image.load_img(\"../results/jackknife/all/mean_jk.nii.gz\")\n",
    "p = plotting.plot_glass_brain(None, display_mode=\"lyrz\", colorbar=True)\n",
    "p.add_overlay(img_jk_all, colorbar=True, cmap=\"RdYlGn\", vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cluster table example\n",
    "t = reporting.get_clusters_table(img_jk_all, stat_threshold=0, min_distance=1000)\n",
    "display(t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
